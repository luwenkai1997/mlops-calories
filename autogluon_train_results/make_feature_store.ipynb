{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73304705-1882-460c-9cdd-cc969b5a8ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/calories.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"/\")\n",
    "DATA_PATH = BASE_DIR / \"calories.csv\"\n",
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7963e54a-6fcd-4385-9f7e-dfaa3d2b4512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb524fe-d2b9-43a9-a600-29e16267f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "raw path s3://myfeaturestore-000/feature-store/features_raw/dt=2025-08-18/\n",
      "std path s3://myfeaturestore-000/feature-store/features_std/dt=2025-08-18/\n",
      "params json s3://myfeaturestore-000/feature-store/params/cal_v1/std_v1.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# S3 only feature store: write raw and standardized features to S3 in Parquet, partitioned by dt\n",
    "\n",
    "REGION = \"us-east-1\"\n",
    "S3_BUCKET = \"myfeaturestore-000\"\n",
    "S3_PREFIX = \"feature-store\"\n",
    "CSV_PATH = \"/home/ec2-user/SageMaker/calories.csv\"\n",
    "\n",
    "FEATURE_SET = \"cal_v1\"\n",
    "STD_VERSION = \"v1\"\n",
    "LABEL_COL = \"Calories\"\n",
    "ENTITY_KEY = \"entity_id\"\n",
    "EVENT_TIME_COL = \"event_time\"\n",
    "\n",
    "ENABLE_GLUE_ATHENA = False\n",
    "GLUE_DB = \"fs\"\n",
    "RAW_TABLE = \"features_raw\"\n",
    "STD_TABLE = \"features_std\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "\n",
    "session = boto3.Session(region_name=REGION)\n",
    "s3 = session.client(\"s3\")\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "def iso_now():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def dt_today():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def to_parquet(df, bucket, base_key, dt):\n",
    "    key = f\"{base_key}/dt={dt}/part-{uuid.uuid4().hex}.parquet\"\n",
    "    with fs.open(f\"{bucket}/{key}\", \"wb\") as f:\n",
    "        pq.write_table(pa.Table.from_pandas(df, preserve_index=False), f)\n",
    "    return f\"s3://{bucket}/{key}\"\n",
    "\n",
    "def athena_type(s):\n",
    "    if pd.api.types.is_integer_dtype(s) or pd.api.types.is_bool_dtype(s):\n",
    "        return \"bigint\"\n",
    "    if pd.api.types.is_float_dtype(s):\n",
    "        return \"double\"\n",
    "    return \"string\"\n",
    "\n",
    "def create_table_sql(table, location_s3, df_for_schema):\n",
    "    cols = [f\"  `{c}` {athena_type(df_for_schema[c])}\" for c in df_for_schema.columns if c != \"dt\"]\n",
    "    cols_sql = \",\\n\".join(cols)\n",
    "    return f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {table} (\n",
    "{cols_sql}\n",
    ")\n",
    "PARTITIONED BY (dt string)\n",
    "STORED AS PARQUET\n",
    "LOCATION '{location_s3}'\n",
    "TBLPROPERTIES ('parquet.compress'='SNAPPY');\n",
    "\"\"\".strip()\n",
    "\n",
    "def run_athena(sql, db, athena, results_bucket):\n",
    "    out = f\"s3://{results_bucket}/athena-results/\"\n",
    "    qid = athena.start_query_execution(\n",
    "        QueryString=sql,\n",
    "        QueryExecutionContext={\"Database\": db},\n",
    "        ResultConfiguration={\"OutputLocation\": out},\n",
    "    )[\"QueryExecutionId\"]\n",
    "    while True:\n",
    "        st = athena.get_query_execution(QueryExecutionId=qid)[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if st in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "            if st != \"SUCCEEDED\":\n",
    "                reason = athena.get_query_execution(QueryExecutionId=qid)[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"\")\n",
    "                raise RuntimeError(f\"Athena query failed: {reason}\")\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "def main():\n",
    "    p = Path(CSV_PATH)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"not found: {CSV_PATH}\")\n",
    "\n",
    "    df0 = pd.read_csv(p)\n",
    "\n",
    "    if \"User_ID\" in df0.columns:\n",
    "        df0[ENTITY_KEY] = df0[\"User_ID\"].astype(int)\n",
    "        base = df0.drop(columns=[\"User_ID\"]).copy()\n",
    "    else:\n",
    "        df0[ENTITY_KEY] = np.arange(1, len(df0) + 1, dtype=int)\n",
    "        base = df0.copy()\n",
    "\n",
    "    if LABEL_COL not in base.columns:\n",
    "        raise ValueError(f\"missing label column {LABEL_COL}\")\n",
    "    base[LABEL_COL] = base[LABEL_COL].astype(float)\n",
    "\n",
    "    if \"Gender\" in base.columns:\n",
    "        base = pd.get_dummies(base, columns=[\"Gender\"], drop_first=True)\n",
    "\n",
    "    train_df, _ = train_test_split(base, test_size=0.2, random_state=42)\n",
    "    numeric_cols = [c for c in base.columns if c not in [LABEL_COL, ENTITY_KEY]]\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "\n",
    "    df_std = base.copy()\n",
    "    df_std[numeric_cols] = scaler.transform(df_std[numeric_cols])\n",
    "\n",
    "    raw = base[[c for c in base.columns if c != LABEL_COL]].copy()\n",
    "    std = df_std[[c for c in df_std.columns if c != LABEL_COL]].copy()\n",
    "\n",
    "    ts = iso_now()\n",
    "    raw[EVENT_TIME_COL] = ts\n",
    "    std[EVENT_TIME_COL] = ts\n",
    "    raw[\"feature_set\"] = FEATURE_SET\n",
    "    std[\"feature_set\"] = FEATURE_SET\n",
    "    std[\"std_version\"] = STD_VERSION\n",
    "\n",
    "    dt = dt_today()\n",
    "    raw_base = f\"{S3_PREFIX}/features_raw\"\n",
    "    std_base = f\"{S3_PREFIX}/features_std\"\n",
    "\n",
    "    raw_uri = to_parquet(raw, S3_BUCKET, raw_base, dt)\n",
    "    std_uri = to_parquet(std, S3_BUCKET, std_base, dt)\n",
    "\n",
    "    params_key = f\"{S3_PREFIX}/params/{FEATURE_SET}/std_{STD_VERSION}.json\"\n",
    "    s3.put_object(\n",
    "        Bucket=S3_BUCKET,\n",
    "        Key=params_key,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"feature_set\": FEATURE_SET,\n",
    "                \"std_version\": STD_VERSION,\n",
    "                \"numeric_cols\": numeric_cols,\n",
    "                \"mean\": scaler.mean_.tolist(),\n",
    "                \"scale\": scaler.scale_.tolist(),\n",
    "                \"saved_at\": ts,\n",
    "            }\n",
    "        ).encode(\"utf-8\"),\n",
    "    )\n",
    "\n",
    "    manifest = {\n",
    "        \"feature_set\": FEATURE_SET,\n",
    "        \"dt\": dt,\n",
    "        \"raw_s3_path\": f\"s3://{S3_BUCKET}/{raw_base}/dt={dt}/\",\n",
    "        \"std_s3_path\": f\"s3://{S3_BUCKET}/{std_base}/dt={dt}/\",\n",
    "        \"raw_example_file\": raw_uri,\n",
    "        \"std_example_file\": std_uri,\n",
    "        \"entity_key\": ENTITY_KEY,\n",
    "        \"event_time_col\": EVENT_TIME_COL,\n",
    "        \"std_version\": STD_VERSION,\n",
    "    }\n",
    "    s3.put_object(\n",
    "        Bucket=S3_BUCKET,\n",
    "        Key=f\"{S3_PREFIX}/manifests/{FEATURE_SET}/{dt}.json\",\n",
    "        Body=json.dumps(manifest, indent=2).encode(\"utf-8\"),\n",
    "    )\n",
    "\n",
    "    if ENABLE_GLUE_ATHENA:\n",
    "        glue = session.client(\"glue\")\n",
    "        athena = session.client(\"athena\")\n",
    "        try:\n",
    "            glue.get_database(Name=GLUE_DB)\n",
    "        except glue.exceptions.EntityNotFoundException:\n",
    "            glue.create_database(DatabaseInput={\"Name\": GLUE_DB})\n",
    "        raw_loc = f\"s3://{S3_BUCKET}/{raw_base}/\"\n",
    "        std_loc = f\"s3://{S3_BUCKET}/{std_base}/\"\n",
    "        run_athena(f\"CREATE DATABASE IF NOT EXISTS {GLUE_DB}\", \"default\", athena, S3_BUCKET)\n",
    "        run_athena(create_table_sql(f\"{GLUE_DB}.{RAW_TABLE}\", raw_loc, raw), GLUE_DB, athena, S3_BUCKET)\n",
    "        run_athena(create_table_sql(f\"{GLUE_DB}.{STD_TABLE}\", std_loc, std), GLUE_DB, athena, S3_BUCKET)\n",
    "        run_athena(f\"MSCK REPAIR TABLE {RAW_TABLE}\", GLUE_DB, athena, S3_BUCKET)\n",
    "        run_athena(f\"MSCK REPAIR TABLE {STD_TABLE}\", GLUE_DB, athena, S3_BUCKET)\n",
    "\n",
    "    print(\"done\")\n",
    "    print(\"raw path\", f\"s3://{S3_BUCKET}/{raw_base}/dt={dt}/\")\n",
    "    print(\"std path\", f\"s3://{S3_BUCKET}/{std_base}/dt={dt}/\")\n",
    "    print(\"params json\", f\"s3://{S3_BUCKET}/{params_key}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe0a41-b801-4155-84cc-c05d7cb2d8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

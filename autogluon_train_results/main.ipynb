{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c095f8-1cbd-4dc0-bdb6-8e39eb9572af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Jul 15 10:36:12 UTC 2025\n",
      "CPU Count:          2\n",
      "Memory Avail:       1.57 GB / 3.76 GB (41.9%)\n",
      "Disk Space Avail:   2.56 GB / 4.78 GB (53.5%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 75s of the 300s of remaining time (25%).\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1444: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.10.0,<2.45.0\"`')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"/home/ec2-user/SageMaker/autogluon_unscaled/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setup Complete ---\n",
      "\n",
      "--- Step 1: Preparing data ---\n",
      "Data loaded successfully. Shape: (15000, 9)\n",
      "--> 'User_ID' column has been excluded from the dataset.\n",
      "Unscaled data prepared. Train shape: (12000, 8)\n",
      "Scaled data prepared. Train shape: (12000, 8)\n",
      "\n",
      "--- Saving test data to local CSV files ---\n",
      "--- Data preparation complete ---\n",
      "\n",
      "--- Step 2: Starting AutoML training ---\n",
      "\n",
      "--- Training (1/2): Unscaled Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 75s\n",
      "AutoGluon will save models to \"/home/ec2-user/SageMaker/autogluon_unscaled/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    10666\n",
      "Train Data Columns: 7\n",
      "Label Column:       Calories\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1615.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.50 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])  : 1 | ['Gender_male']\n",
      "\t\t('float', []) : 5 | ['Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
      "\t\t('int', [])   : 1 | ['Age']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 5 | ['Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
      "\t\t('int', [])       : 1 | ['Age']\n",
      "\t\t('int', ['bool']) : 1 | ['Gender_male']\n",
      "\t0.1s = Fit runtime\n",
      "\t7 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.50 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 49.92s of the 74.89s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.10.0,<2.45.0\"`\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 45.74s of the 70.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 45.44s of the 70.41s of remaining time.\n",
      "\t-3.0098\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.66s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 35.59s of the 60.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 35.23s of the 60.19s of remaining time.\n",
      "\t-2.5985\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.58s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 29.46s of the 54.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 29.15s of the 54.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 28.85s of the 53.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 6)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 6)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\t-3.1858\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.05s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1.56s of the 26.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1.24s of the 26.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 0.93s of the 25.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tNot enough time to train first epoch. Stopped on Update 9 (Epoch 0))\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L1.\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 0.53s of the 25.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 0.22s of the 25.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 74.91s of the 24.54s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.562, 'NeuralNetTorch_BAG_L1': 0.375, 'RandomForestMSE_BAG_L1': 0.062}\n",
      "\t-2.2039\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 24.41s of the 24.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 24.03s of the 24.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 23.53s of the 23.50s of remaining time.\n",
      "\t-1.808\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.98s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 5.51s of the 5.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 5.18s of the 5.15s of remaining time.\n",
      "\t-1.7596\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.19s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 74.91s of the -1.25s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L2': 0.8, 'RandomForestMSE_BAG_L2': 0.16, 'NeuralNetTorch_BAG_L1': 0.04}\n",
      "\t-1.7567\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 76.38s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 3855.9 rows/s (1334 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/ec2-user/SageMaker/autogluon_unscaled/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L3      -1.462047  -1.756744  root_mean_squared_error        3.620798       2.212940  62.478184                 0.002819                0.000770           0.018158            3       True          7\n",
      "1    ExtraTreesMSE_BAG_L2      -1.469529  -1.759551  root_mean_squared_error        2.667457       1.738526  45.479953                 0.303056                0.501252           5.188097            2       True          6\n",
      "2  RandomForestMSE_BAG_L2      -1.535395  -1.808003  root_mean_squared_error        3.314923       1.710917  57.271929                 0.950522                0.473644          16.980073            2       True          5\n",
      "3     WeightedEnsemble_L2      -2.084087  -2.203899  root_mean_squared_error        2.368184       1.238081  40.305990                 0.003784                0.000808           0.014134            2       True          4\n",
      "4   NeuralNetTorch_BAG_L1      -2.562674  -3.185784  root_mean_squared_error        0.100480       0.079077  27.054157                 0.100480                0.079077          27.054157            1       True          3\n",
      "5    ExtraTreesMSE_BAG_L1      -2.570068  -2.598541  root_mean_squared_error        1.480185       0.599901   4.576535                 1.480185                0.599901           4.576535            1       True          2\n",
      "6  RandomForestMSE_BAG_L1      -2.966960  -3.009794  root_mean_squared_error        0.783736       0.558295   8.661165                 0.783736                0.558295           8.661165            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t81s\t = DyStack   runtime |\t219s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 219s\n",
      "AutoGluon will save models to \"/home/ec2-user/SageMaker/autogluon_unscaled\"\n",
      "Train Data Rows:    12000\n",
      "Train Data Columns: 7\n",
      "Label Column:       Calories\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1211.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])  : 1 | ['Gender_male']\n",
      "\t\t('float', []) : 5 | ['Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
      "\t\t('int', [])   : 1 | ['Age']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 5 | ['Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
      "\t\t('int', [])       : 1 | ['Age']\n",
      "\t\t('int', ['bool']) : 1 | ['Gender_male']\n",
      "\t0.1s = Fit runtime\n",
      "\t7 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.56 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 145.82s of the 218.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 145.47s of the 218.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 145.13s of the 218.09s of remaining time.\n",
      "\t-2.8382\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.79s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 132.86s of the 205.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 132.50s of the 205.45s of remaining time.\n",
      "\t-2.5391\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.68s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 124.30s of the 197.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 123.96s of the 196.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 123.62s of the 196.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 32)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 43)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 55)\n",
      "\t-2.4018\t = Validation score   (-root_mean_squared_error)\n",
      "\t96.53s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 26.85s of the 99.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 26.53s of the 99.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 26.20s of the 99.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 6)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 12)\n",
      "\t-4.3893\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.64s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1.31s of the 74.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 0.99s of the 73.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 0.65s of the 73.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 0.31s of the 73.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 218.79s of the 72.78s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.409, 'RandomForestMSE_BAG_L1': 0.045, 'NeuralNetTorch_r79_BAG_L1': 0.045}\n",
      "\t-1.9137\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 72.66s of the 72.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 72.31s of the 72.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 71.97s of the 71.93s of remaining time.\n",
      "\t-1.5\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.19s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 49.84s of the 49.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 49.50s of the 49.47s of remaining time.\n",
      "\t-1.4475\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.11s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 42.32s of the 42.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 41.98s of the 41.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 41.63s of the 41.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 15)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 10)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 15)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 19)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 20)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 16)\n",
      "\t-2.7261\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.48s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 218.79s of the 1.74s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L2': 0.84, 'RandomForestMSE_BAG_L2': 0.12, 'NeuralNetTorch_BAG_L2': 0.04}\n",
      "\t-1.4451\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 217.3s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2652.8 rows/s (1500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/ec2-user/SageMaker/autogluon_unscaled\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Jul 15 10:36:12 UTC 2025\n",
      "CPU Count:          2\n",
      "Memory Avail:       1.24 GB / 3.76 GB (32.9%)\n",
      "Disk Space Avail:   1.71 GB / 4.78 GB (35.9%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 75s of the 300s of remaining time (25%).\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1444: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.10.0,<2.45.0\"`')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"/home/ec2-user/SageMaker/autogluon_scaled/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training (2/2): Scaled Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 75s\n",
      "AutoGluon will save models to \"/home/ec2-user/SageMaker/autogluon_scaled/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    10666\n",
      "Train Data Columns: 7\n",
      "Label Column:       Calories\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1266.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.50 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])  : 1 | ['Gender_male']\n",
      "\t\t('float', []) : 6 | ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 6 | ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Gender_male']\n",
      "\t0.1s = Fit runtime\n",
      "\t7 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.50 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 49.92s of the 74.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 49.58s of the 74.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 49.24s of the 74.20s of remaining time.\n",
      "\t-3.0076\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.79s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 39.36s of the 64.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 39.04s of the 64.00s of remaining time.\n",
      "\t-2.5985\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.5s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 33.36s of the 58.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 33.03s of the 57.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 32.68s of the 57.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 10)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\t-2.6789\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.03s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1.44s of the 26.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1.11s of the 26.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 0.80s of the 25.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tNot enough time to train first epoch. Stopped on Update 9 (Epoch 0))\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L1.\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 0.42s of the 25.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 0.11s of the 25.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.0s)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r191_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 74.91s of the 24.61s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.48, 'NeuralNetTorch_BAG_L1': 0.48, 'RandomForestMSE_BAG_L1': 0.04}\n",
      "\t-2.0649\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 24.49s of the 24.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 24.15s of the 24.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 23.84s of the 23.81s of remaining time.\n",
      "\t-1.685\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.44s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 6.43s of the 6.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 6.10s of the 6.08s of remaining time.\n",
      "\t-1.6422\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.15s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 74.91s of the -0.28s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L2': 0.76, 'RandomForestMSE_BAG_L2': 0.2, 'NeuralNetTorch_BAG_L1': 0.04}\n",
      "\t-1.6382\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 75.43s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 4088.6 rows/s (1334 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/ec2-user/SageMaker/autogluon_scaled/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L3      -1.343915  -1.638161  root_mean_squared_error        1.577433       2.142373  65.927666                 0.002557                0.000736           0.023103            3       True          7\n",
      "1    ExtraTreesMSE_BAG_L2      -1.352888  -1.642205  root_mean_squared_error        1.280394       1.653918  49.467716                 0.289639                0.506230           5.153306            2       True          6\n",
      "2  RandomForestMSE_BAG_L2      -1.386621  -1.685002  root_mean_squared_error        1.285237       1.635406  60.751256                 0.294482                0.487719          16.436847            2       True          5\n",
      "3     WeightedEnsemble_L2      -1.940588  -2.064914  root_mean_squared_error        0.993169       1.148452  44.328170                 0.002414                0.000765           0.013761            2       True          4\n",
      "4   NeuralNetTorch_BAG_L1      -2.265683  -2.678891  root_mean_squared_error        0.085283       0.066660  31.029084                 0.085283                0.066660          31.029084            1       True          3\n",
      "5    ExtraTreesMSE_BAG_L1      -2.570068  -2.598540  root_mean_squared_error        0.462645       0.556617   4.496778                 0.462645                0.556617           4.496778            1       True          2\n",
      "6  RandomForestMSE_BAG_L1      -2.958120  -3.007558  root_mean_squared_error        0.442827       0.524410   8.788548                 0.442827                0.524410           8.788548            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t78s\t = DyStack   runtime |\t222s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 222s\n",
      "AutoGluon will save models to \"/home/ec2-user/SageMaker/autogluon_scaled\"\n",
      "Train Data Rows:    12000\n",
      "Train Data Columns: 7\n",
      "Label Column:       Calories\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1386.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])  : 1 | ['Gender_male']\n",
      "\t\t('float', []) : 6 | ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 6 | ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Gender_male']\n",
      "\t0.1s = Fit runtime\n",
      "\t7 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.56 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 148.16s of the 222.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 147.82s of the 221.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 147.48s of the 221.59s of remaining time.\n",
      "\t-2.8306\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.93s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 136.42s of the 210.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 136.09s of the 210.20s of remaining time.\n",
      "\t-2.539\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.02s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 129.88s of the 203.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 129.55s of the 203.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 129.20s of the 203.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 44)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 38)\n",
      "\t-2.4018\t = Validation score   (-root_mean_squared_error)\n",
      "\t98.19s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 30.80s of the 104.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 30.47s of the 104.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 30.13s of the 104.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\t-4.0429\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.56s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1.32s of the 75.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 0.99s of the 75.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 0.66s of the 74.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 0.32s of the 74.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 222.30s of the 73.92s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.375, 'RandomForestMSE_BAG_L1': 0.083, 'NeuralNetTorch_r79_BAG_L1': 0.042}\n",
      "\t-1.9193\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 73.79s of the 73.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 73.42s of the 73.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 73.09s of the 73.06s of remaining time.\n",
      "\t-1.4996\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.1s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 51.05s of the 51.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 50.71s of the 50.68s of remaining time.\n",
      "\t-1.4292\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.11s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 43.45s of the 43.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 43.11s of the 43.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 42.75s of the 42.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 15)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 16)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 16)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 18)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 20)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 21)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 20)\n",
      "\t-2.6766\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.44s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 222.30s of the 1.85s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L2': 0.96, 'NeuralNetTorch_BAG_L2': 0.04}\n",
      "\t-1.4276\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 220.69s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 3428.7 rows/s (1500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/ec2-user/SageMaker/autogluon_scaled\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AutoML training complete ---\n"
     ]
    }
   ],
   "source": [
    "#%%!pip install autogluon.tabular --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "print(\"--- Setup Complete ---\")\n",
    "\n",
    "# Step 1: Data Preparation (Excluding User_ID)\n",
    "print(\"\\n--- Step 1: Preparing data ---\")\n",
    "#### change your path\n",
    "try:\n",
    "    df = pd.read_csv('autogluon_train_results/raw_data/calories.csv')\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'autogluon_train_results/raw_data/calories.csv' file not found.\")\n",
    "    raise\n",
    "\n",
    "# --- NEW: Exclude User_ID before any other processing ---\n",
    "df = df.drop(columns=['User_ID'])\n",
    "print(\"--> 'User_ID' column has been excluded from the dataset.\")\n",
    "\n",
    "df['Calories'] = df['Calories'].astype(float)\n",
    "df_encoded = pd.get_dummies(df, columns=['Gender'], drop_first=True)\n",
    "\n",
    "# Version 1: Unscaled Data\n",
    "train_df_unscaled, test_df_unscaled = train_test_split(df_encoded, test_size=0.2, random_state=42)\n",
    "print(f\"Unscaled data prepared. Train shape: {train_df_unscaled.shape}\")\n",
    "\n",
    "# Version 2: Scaled Data\n",
    "numeric_cols = [c for c in df_encoded.columns if c not in ['Calories', 'Gender_male']]\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df_encoded.copy()\n",
    "df_scaled[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "train_df_scaled, test_df_scaled = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
    "print(f\"Scaled data prepared. Train shape: {train_df_scaled.shape}\")\n",
    "\n",
    "# --- Save the correct test sets for the validation script ---\n",
    "print(\"\\n--- Saving test data to local CSV files ---\")\n",
    "test_df_unscaled.to_csv('test_data_unscaled_noID.csv', index=False)\n",
    "test_df_scaled.to_csv('test_data_scaled_noID.csv', index=False)\n",
    "\n",
    "print(\"--- Data preparation complete ---\")\n",
    "\n",
    "# Step 2: AutoML Training\n",
    "print(\"\\n--- Step 2: Starting AutoML training ---\")\n",
    "\n",
    "print(\"\\n--- Training (1/2): Unscaled Data ---\")\n",
    "predictor_unscaled = TabularPredictor(\n",
    "    label='Calories', problem_type='regression', eval_metric='root_mean_squared_error', path='autogluon_unscaled'\n",
    ").fit(TabularDataset(train_df_unscaled), presets='best_quality', time_limit=300)\n",
    "\n",
    "print(\"\\n--- Training (2/2): Scaled Data ---\")\n",
    "predictor_scaled = TabularPredictor(\n",
    "    label='Calories', problem_type='regression', eval_metric='root_mean_squared_error', path='autogluon_scaled'\n",
    ").fit(TabularDataset(train_df_scaled), presets='best_quality', time_limit=300)\n",
    "\n",
    "print(\"--- AutoML training complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd0ad6-4189-4604-92b7-ff0a255dc6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d5408-7a98-4f65-98b0-62aaf334837f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb4b84-f52a-436e-95d4-f1c884a64e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9cc1b8-35d5-4a76-9e9d-1da521a26bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75495c-4bdb-41d4-af0e-fdc1666158b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
